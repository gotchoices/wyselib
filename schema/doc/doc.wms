# Store and retrieve documents
#Copyright WyattERP.org; See license in root of this package
#----------------------------------------------------------------
#TODO:
#X- Make unique on path, name
#X- Implement update trigger, replace large object if necessary
#X- Implement delete trigger
#X- Can't update/delete a locked object
#X- Delete (and archive) large objects that are not found in doc.doc
#X- Create maintenance stored procedure to check file/object integrity
#X-  Recover from lost large object
#X-  Recover from lost or corrupted external file
#X- Set date on archive files to correspond with database last modified date
#- System parameter to tell how many files to verify per pass
#- Implement verify routine
#- 

schema doc -grant public
sequence doc.doc_seq {doc} {minvalue 1000}

# Contains an entry for each document
#----------------------------------------------------------------
table doc.doc {doc.doc_seq base.ent} {
    id		int		primary key default nextval('doc.doc_seq')
  , path	varchar[]	not null constraint "doc.doc.IPS" check (path[1] is not null and path[1] != '' and substring(path[1],0,1) != '.')
  , name	varchar		not null constraint "doc.doc.IFN" check (name != '' and name ~ '[^\\*/<>()]' and substring(name,0,1) != '.')
  , version	int		not null constraint "doc.doc.IVN" check (version >= 0)
  , exten	varchar		not null default ''
  , cmt		varchar
  , locked	boolean		default 'f'
  , size	bigint		not null
  , cksum	bigint		not null
  , blob	oid		not null unique
  , checked	timestamptz
  , constraint "!doc.doc.UFN"	unique	(path,name,version,exten)
    subst($glob::stamps)
}

# Note how some documents depend on others
#----------------------------------------------------------------
table doc.link {doc.doc} {
    outp	int		references doc.doc on update cascade on delete cascade
  , inp		int		references doc.doc on update cascade
  , type	varchar		not null constraint "doc.link.ITP" check (type in ('auto','link','manu','rend'))
}

# Basic view
#----------------------------------------------------------------
view doc.doc_v {doc.doc} {select
    eval(fld_list $doc::doc_se d)
  , array_to_string(d.path,'/') || '/' || d.name || case when d.version <= 0 then '' else '(' || d.version || ')' end || '.' || d.exten	as relname
  , coalesce(lo.blocks,0)	as blocks
  , lo.bytes
    from	doc.doc		d
    left join	(select loid,count(*) as blocks,sum(octet_length(data)) as bytes from pg_largeobject group by 1) lo on lo.loid = d.blob
}

# When inserting a new document
#----------------------------------------------------------------
function doc.doc_tf_ins() {doc.doc doc.archive(doc.doc,int)} {
  returns trigger language plpgsql as $$
    declare
        tiar	bigint[];
    begin
-- raise notice 'doc_ins:%', new.id;
        if new.version is null then
            new.version := (select coalesce(max(version)+1,0) from doc.doc where path = new.path and name = new.name and exten = new.exten);
        end if;
        tiar := doc.archive(new,1);
        new.size := tiar[1];
        new.cksum := tiar[2];
        return new;
    end;
$$;}
trigger doc_doc_tr_ins {} {before insert on doc.doc for each row execute procedure doc.doc_tf_ins();}

# When updating a document
#----------------------------------------------------------------
function doc.doc_tf_upd() {doc.doc doc.archive(doc.doc,int)} {
  returns trigger language plpgsql as $$
    declare
        tiar	bigint[];
    begin
        if session_user != 'dbadmin()' and old.locked then
            raise exception '!doc.doc.DLK %', old.id;	-- Can't modify locked docs
        end if;

        if not eval(samelist path name version exten) then	-- if we are changing where the file is archived
            if exists (select * from doc.doc where path = new.path and name = new.name and version = new.version and exten = new.exten) then
                raise exception '!doc.doc.CFN % % % %', new.path, new.name, new.version, new.exten;		-- Conflicting record found
            end if;
            tiar := doc.archive(new, 1);
            perform doc.archive(old, 0);
        elseif new.blob != old.blob then			-- same archive name but new file contents
            tiar := doc.archive(new, 1);
            perform lo_unlink(old.blob);			-- get rid of the old large object
        elseif not eval(samelist size cksum) then		-- if trying to change size or checksum
            tiar := doc.archive(new, 1);			-- rearchive, fresh copy
        end if;
        if tiar is not null then				-- if size,cksum got modified
            new.size  := tiar[1];
            new.cksum := tiar[2];
        end if;
        return new;
    end;
$$;}
trigger doc_doc_tr_upd {} {before update on doc.doc for each row execute procedure doc.doc_tf_upd();}

# When deleting a document
#----------------------------------------------------------------
function doc.doc_tf_del() {doc.doc doc.archive(doc.doc,int)} {
  returns trigger language plpgsql as $$
    begin
        if session_user != 'dbadmin()' and old.locked then
            raise exception '!doc.doc.DLK %', old.id;	-- Can't modify locked docs
        end if;
        perform doc.archive(old, 0);
        perform lo_unlink(old.blob);			-- get rid of the old large object
        return old;
    end;
$$;}
trigger doc_doc_tr_del {} {before delete on doc.doc for each row execute procedure doc.doc_tf_del();}

# Store (or delete) a backup copy of the large object in the filesystem
# $1: doc record (new/old)
# $2: 0=delete, 1=store
#----------------------------------------------------------------
function doc.archive(doc.doc,int) {doc.doc} {
  returns bigint[] language pltclu as $$
 
    set path "subst($doc::doc_root)/[join [split [lindex $1(path) 0] ,] /]"
    if {![file exists $path]} {file mkdir $path}
    set base "$1(name)"
    if {$1(version) > 0} {append base "($1(version))"}
    if {$1(exten) != {}} {append base ".$1(exten)"}
    set fname "$path/$base"
#elog NOTICE "id:$1(id) path:$path fname:$fname"
    if {!$2} {				;#if deleting
        if {"subst($doc::doc_del)" != {}} {		;#if a delete directory exists
            set dpath "subst($doc::doc_del)/[join [split [lindex $1(path) 0] ,] /]"
            if {![file exists $dpath]} {file mkdir $dpath}
            set suff {}
            set sver 0
            while {[file exists $dpath/$base$suff]} {set suff "-[incr sver]"}
            file rename -force $fname $dpath/$base$suff
        } else {
            file delete -force $fname
        }
        return_null
    }

    spi_exec "select lo_export($1(blob),'$fname')"
    file mtime $fname [clock scan $1(mod_date)]
    lassign [exec subst($doc::cksum) $fname] cksum size
#elog NOTICE " cksum:$cksum size:$size"
    return "{$size,$cksum}"
$$;}

# Check the integrity of the document database
#----------------------------------------------------------------
# lg_obj	doc	arch	case	action
#     no	 no	 no	0	N/A
#     no	 no	yes	1	Failed Trans	Log and move to Stray
#     no	yes	 no	2	Lost file/arch	Log and return error
#     no	yes	yes	3	New DB Regen	Log and import doc to LO
#    yes	 no	  ?	4,5	Orphan		Log and save file by its OID in Orphan (assuming doc is the only system using pg_largeobject)
#    yes	yes	 no	6	File Damage	Log and re-write archive
#    yes	yes	yes	7	OK		Check doc against LO size and cksum
#----------------------------------------------------------------
function doc.vacuum() {doc.doc} {
  returns varchar language pltclu security definer as $$

    proc date_fmt {secs} {return "'[clock format $secs -format {%Y-%b-%d_%H:%M:%S}]'"}
    
    #----------------------------------------------------------------
    proc logit {fp msg} {					;#post to the log file
        puts $fp "[date_fmt [clock seconds]]: $msg"
    }

    # Fill temporary table with archive file information from a single folder
    #----------------------------------------------------------------
    proc check_folder {root dir} {				;#look at all the archive files in dir
        upvar lf lf

        set rdir "$root/$dir"
        set empty 1
        foreach sub [glob -nocomplain -tails -types d -directory $rdir *] {
            if {$sub == {.} || $sub == {..}} continue
            check_folder $root $dir/$sub			;#recurse through sub-directories
            set empty 0
        }
logit $lf "Folder: $root $dir"
        set names {}
        foreach name [glob -nocomplain -tails -types f -directory $rdir * .*] {
            set empty 0
            set fsize [file size $rdir/$name]
            set fmod [date_fmt [file mtime $rdir/$name]]
            spi_exec "insert into doc_arch (dirname,filename,fsize,fmod) values ('$dir', '$name', $fsize, $fmod)"
        }
        if {$empty} {
            logit $lf "Removing empty directory: $rdir"
            file delete $rdir
        }
    }

    # Main
    #----------------------------------------------------------------
    set lf [open subst($doc::logfile) a]
    logit $lf {Starting}
    set result {}

    set dbg {temporary}		;#set dbg {}		;#uncomment for debugging	
    spi_exec "
        create $dbg table if not exists doc_arch (dirname varchar, filename varchar, fsize bigint, fmod timestamp);
        create or replace $dbg view doc_arch_v as select *, dirname || '/' || filename as relname_r from doc_arch;
        create or replace $dbg view doc_join_v as select * from doc.doc_v d full join doc_arch_v a on a.relname_r = d.relname;
--        delete from doc_arch;				-- use if in debug mode
    "
    foreach dir [glob -nocomplain -tails -directory subst($doc::doc_root) -types d *] {
        check_folder subst($doc::doc_root) $dir
    }

    set errs {}				;# Case 1: apparent aborted transactions - move to trash
    #----------------------------------------------------------------
    spi_exec -array r {select * from doc_join_v where bytes is null and id is null and fsize is not null order by relname_r} {
        lappend errs "  $r(relname_r)"
        set dest subst($doc::doc_stray)/$r(relname_r)
        set ddir [file dirname $dest]
        if {![file exists $ddir]} {file mkdir $ddir}
        file rename -force subst($doc::doc_root)/$r(relname_r) $dest
    }
    if {[llength $errs] > 0} {append result "\n Strays in archive (-> subst($doc::doc_stray)):\n[join $errs "\n"]"}

    set errs {}				;# Find case 2 - Unrecoverable
    #----------------------------------------------------------------
    spi_exec -array r {select * from doc_join_v where bytes is null and id is not null and fsize is null order by relname} {
        lappend errs "  Doc:$r(id), $r(relname)"
    }
    if {[llength $errs] > 0} {append result "\n Missing large object and archive (Consult backups!):\n[join $errs "\n"]"}
    
    set errs {}				;# Find case 3 - Reconstruct large object from archive
    #----------------------------------------------------------------
    spi_exec -array r {select * from doc_join_v where bytes is null and id is not null and fsize is not null order by relname limit base.parm('doc','rebuild',subst($doc::doc_rebuild)::int)} {
        lappend errs "  $r(relname)"
        set arch subst($doc::doc_root)/$r(relname_r)
        spi_exec "
             alter table doc.doc disable trigger doc_doc_tr_upd;		-- so we don't re-archive right away
             update doc.doc set blob = lo_import('$arch') where id = $r(id);
             alter table doc.doc enable trigger doc_doc_tr_upd;
         "
    }
    if {[llength $errs] > 0} {append result "\n Missing large objects (Re-constructing):\n[join $errs "\n"]"}
    
    set errs {}				;# Case 4,5 - orphans
    #----------------------------------------------------------------
    spi_exec -array r {select lo.loid from pg_largeobject lo where pageno = 0 and not exists (select * from doc.doc where blob = lo.loid)} {
        set dest subst($doc::doc_orphan)/$r(loid)
        lappend errs "  $r(loid) -> $dest"
        set ddir [file dirname $dest]
        if {![file exists $ddir]} {file mkdir $ddir}
        spi_exec "select lo_export($r(loid),'$dest')"
        spi_exec "select lo_unlink($r(loid));"
    }
    if {[llength $errs] > 0} {append result "\n Orphan large objects (-> subst($doc::doc_orphan)):\n[join $errs "\n"]"}

    set errs {}				;# Find case 6 - Reconstruct archive
    #----------------------------------------------------------------
    spi_exec -array r {select * from doc_join_v where bytes is not null and id is not null and fsize is null order by relname limit base.parm('doc','archive',subst($doc::doc_archive)::int)} {
        lappend errs "  Doc:$r(id), $r(relname)"
        spi_exec "update doc.doc set size = null where id = $r(id);"	;# force re-archive
    }
    if {[llength $errs] > 0} {append result "\n Missing archive (Re-saved):\n[join $errs "\n"]"}
    
    set errs {}				;# Find case F - Verify
    #----------------------------------------------------------------
    spi_exec -array r {select * from doc_join_v where bytes is not null and id is not null and fsize is not null order by checked nulls first limit base.parm('doc','verify',subst($doc::doc_verify)::int)} {
        spi_exec "select lo_export($r(blob),'subst($doc::doc_tmp)')"
        lassign [exec subst($doc::cksum) subst($doc::doc_tmp)] dc ds
        lassign [exec subst($doc::cksum) subst($doc::doc_root)/$r(relname)] ac as
        file delete subst($doc::doc_tmp)
        if {$dc == $ac && $ds == $as} {
            spi_exec "update doc.doc set checked = current_timestamp where id = $r(id);"
        } else {
            lappend errs "  Doc:$r(id), $r(relname) obcks:$dc arcks:$ac obsiz:$ds arsiz:$as"
        }
    }
    if {[llength $errs] > 0} {append result "\n Verify Failure:\n[join $errs "\n"]"}

    if {$result != {}} {logit $lf "$result"}
    logit $lf Finished
    close $lf
    if {$result == {}} {return {}} else {return "Vacuum:$result"}
$$;}
